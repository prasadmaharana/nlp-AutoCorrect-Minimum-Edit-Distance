{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrect using minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "We use autocorrect every day on our cell phone and computer. In this notebook, we will explore what really goes on behind the scenes. This model implement is not identical to the one used in phones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/auto-correct.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:300px;height:250px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Edit Distance\n",
    "\n",
    "- We say two words are n edit distance away from each other when we need n edits to change one word into another. \n",
    "\n",
    "An edit could consist of one of the following options: \n",
    "\n",
    "- Delete (remove a letter): ‘hat’ => ‘at, ha, ht’\n",
    "- Switch (swap 2 adjacent letters): ‘eta’ => ‘eat, tea,...’\n",
    "- Replace (change 1 letter to another): ‘jat’ => ‘hat, rat, cat, mat, ...’\n",
    "- Insert (add a letter): ‘te’ => ‘the, ten, ate, ...’\n",
    "\n",
    "\n",
    "This auto-correct we are implementing was first created by [Peter Norvig](https://en.wikipedia.org/wiki/Peter_Norvig) in 2007. \n",
    "- [original article](https://norvig.com/spell-correct.html)\n",
    "\n",
    "The goal of our spell check model is to compute the following probability:\n",
    "\n",
    "$$P(c|w) = \\frac{P(w|c)\\times P(c)}{P(w)} \\tag{Eqn-1}$$\n",
    "\n",
    "The equation above is [Bayes Rule](https://en.wikipedia.org/wiki/Bayes%27_theorem). \n",
    "- Equation 1 says that the probability of a word being correct $P(c|w) $is equal to the probability of having a certain word $w$, given that it is correct $P(w|c)$, multiplied by the probability of being correct in general $P(C)$ divided by the probability of that word $w$ appearing $P(w)$ in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in any other machine learning task, the first thing we have to do is process the data set. \n",
    "\n",
    "I am using **'shakespeare.txt'** as my text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Data\n",
    "\n",
    "1) Reads in a corpus (text file)\n",
    "\n",
    "2) Changes everything to lowercase\n",
    "\n",
    "3) Returns a list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in the current directory\n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file) in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    \n",
    "    #Open the file, read its contents into a string variable\n",
    "    with open(file_name,'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # convert all letters to lower case\n",
    "    content = content.lower()\n",
    "    \n",
    "    #Convert every word to lower case and return them in a list.\n",
    "    words = re.findall(r'\\w+', content)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
      "There are 6116 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_l = process_data('./data/shakespeare.txt')\n",
    "vocab = set(word_l)  # new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output\n",
    "```Python\n",
    "The first ten words in the text are: \n",
    "['o', 'for', 'a', 'muse', 'of', 'fire', 'that', 'would', 'ascend', 'the']\n",
    "There are 6116 unique words in the vocabulary.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Count\n",
    "\n",
    "- The dictionary's keys are words\n",
    "- The value for each word is the number of times that word appears in the corpus. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "\n",
    "    for w in word_l:\n",
    "        if w in word_count_dict.keys():\n",
    "            word_count_dict[w] +=1\n",
    "        else:\n",
    "            word_count_dict[w]=1\n",
    "\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6116 key values pairs\n",
      "The count for the word 'thee' is 240\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('thee',0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get probabilty\n",
    "Given the dictionary of word counts, compute the probability that each word will appear if randomly selected from the corpus of words.\n",
    "\n",
    "$$P(w_i) = \\frac{C(w_i)}{M} \\tag{Eqn-2}$$\n",
    "where \n",
    "\n",
    "$C(w_i)$ is the total number of times $w_i$ appears in the corpus.\n",
    "\n",
    "$M$ is the total number of words in the corpus.\n",
    "\n",
    "For example, the probability of the word 'am' in the sentence **'I am happy because I am learning'** is:\n",
    "\n",
    "$$P(am) = \\frac{C(w_i)}{M} = \\frac {2}{7} \\tag{Eqn-3}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "    \n",
    "    # get the total count of words for all words in the dictionary\n",
    "    \n",
    "    sum_w = sum(word_count_dict.values())\n",
    "    \n",
    "    for i,j in word_count_dict.items():\n",
    "        probs[i] = j/sum_w\n",
    "    \n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 6116\n",
      "P('thee') is 0.0045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('thee') is {probs['thee']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulations\n",
    "\n",
    "Now that we have computed $P(w_i)$ for all the words in the corpus, we will write a few functions to manipulate strings so that we can edit the erroneous strings and return the right spellings of the words. \n",
    "\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:]) for i in range (0,len(word)+1)]\n",
    "    delete_l = [L+R[1:] for L,R in split_l if R]\n",
    "    \n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return  delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word cans, \n",
      "split_l = [('', 'cans'), ('c', 'ans'), ('ca', 'ns'), ('can', 's'), ('cans', '')], \n",
      "delete_l = ['ans', 'cns', 'cas', 'can']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"cans\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:]) for i in range (0,len(word)+1)]\n",
    "    switch_l = [L+R[1]+R[0]+R[2:] for L,R in split_l if len(R)>=2]\n",
    "\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "    \n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b5e477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = eta \n",
      "split_l = [('', 'eta'), ('e', 'ta'), ('et', 'a'), ('eta', '')] \n",
      "switch_l = ['tea', 'eat']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"eta\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:]) for i in range (0,len(word)+1)]\n",
    "    replace_l = [L + letter + R[1:] for L, R in split_l if len(R)>0 for letter in letters]\n",
    "    replace_l = [w for w in replace_l if w!=word]\n",
    "\n",
    "    \n",
    "    # Sort the list, for easier viewing\n",
    "    replace_l = sorted(replace_l)\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = can \n",
      "split_l = [('', 'can'), ('c', 'an'), ('ca', 'n'), ('can', '')] \n",
      "replace_l ['aan', 'ban', 'caa', 'cab', 'cac', 'cad', 'cae', 'caf', 'cag', 'cah', 'cai', 'caj', 'cak', 'cal', 'cam', 'cao', 'cap', 'caq', 'car', 'cas', 'cat', 'cau', 'cav', 'caw', 'cax', 'cay', 'caz', 'cbn', 'ccn', 'cdn', 'cen', 'cfn', 'cgn', 'chn', 'cin', 'cjn', 'ckn', 'cln', 'cmn', 'cnn', 'con', 'cpn', 'cqn', 'crn', 'csn', 'ctn', 'cun', 'cvn', 'cwn', 'cxn', 'cyn', 'czn', 'dan', 'ean', 'fan', 'gan', 'han', 'ian', 'jan', 'kan', 'lan', 'man', 'nan', 'oan', 'pan', 'qan', 'ran', 'san', 'tan', 'uan', 'van', 'wan', 'xan', 'yan', 'zan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='can',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i],word[i:]) for i in range (0,len(word)+1)]\n",
    "    insert_l = [L + letter + R for L, R in split_l if len(R)>=0 for letter in letters]\n",
    "\n",
    "    \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word at \n",
      "split_l = [('', 'at'), ('a', 't'), ('at', '')] \n",
      "insert_l = ['aat', 'bat', 'cat', 'dat', 'eat', 'fat', 'gat', 'hat', 'iat', 'jat', 'kat', 'lat', 'mat', 'nat', 'oat', 'pat', 'qat', 'rat', 'sat', 'tat', 'uat', 'vat', 'wat', 'xat', 'yat', 'zat', 'aat', 'abt', 'act', 'adt', 'aet', 'aft', 'agt', 'aht', 'ait', 'ajt', 'akt', 'alt', 'amt', 'ant', 'aot', 'apt', 'aqt', 'art', 'ast', 'att', 'aut', 'avt', 'awt', 'axt', 'ayt', 'azt', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz']\n",
      "Number of strings output by insert_letter('at') is 78\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('at', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Combining the Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = ()\n",
    "\n",
    "    edit_one_set = delete_letter(word) + insert_letter(word) + replace_letter(word)\n",
    "    \n",
    "    if allow_switches:\n",
    "        edit_one_set +=switch_letter(word)\n",
    "    \n",
    "    \n",
    "    # return this as a set and not a list\n",
    "    return set(edit_one_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word at \n",
      "edit_one_l \n",
      "['a', 'aa', 'aat', 'ab', 'abt', 'ac', 'act', 'ad', 'adt', 'ae', 'aet', 'af', 'aft', 'ag', 'agt', 'ah', 'aht', 'ai', 'ait', 'aj', 'ajt', 'ak', 'akt', 'al', 'alt', 'am', 'amt', 'an', 'ant', 'ao', 'aot', 'ap', 'apt', 'aq', 'aqt', 'ar', 'art', 'as', 'ast', 'ata', 'atb', 'atc', 'atd', 'ate', 'atf', 'atg', 'ath', 'ati', 'atj', 'atk', 'atl', 'atm', 'atn', 'ato', 'atp', 'atq', 'atr', 'ats', 'att', 'atu', 'atv', 'atw', 'atx', 'aty', 'atz', 'au', 'aut', 'av', 'avt', 'aw', 'awt', 'ax', 'axt', 'ay', 'ayt', 'az', 'azt', 'bat', 'bt', 'cat', 'ct', 'dat', 'dt', 'eat', 'et', 'fat', 'ft', 'gat', 'gt', 'hat', 'ht', 'iat', 'it', 'jat', 'jt', 'kat', 'kt', 'lat', 'lt', 'mat', 'mt', 'nat', 'nt', 'oat', 'ot', 'pat', 'pt', 'qat', 'qt', 'rat', 'rt', 'sat', 'st', 't', 'ta', 'tat', 'tt', 'uat', 'ut', 'vat', 'vt', 'wat', 'wt', 'xat', 'xt', 'yat', 'yt', 'zat', 'zt']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 129\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"at\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turning this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''\n",
    "    \n",
    "    edit_two_set = set()\n",
    "    tmp=[]\n",
    "    \n",
    "    \n",
    "    edit_one_set = delete_letter(word) + insert_letter(word) + replace_letter(word)   \n",
    "\n",
    "    if allow_switches:\n",
    "        edit_one_set +=switch_letter(word)\n",
    "    \n",
    "    for word in edit_one_set:\n",
    "        tmp += delete_letter(word) + insert_letter(word) + replace_letter(word)\n",
    "        if allow_switches:\n",
    "            tmp += switch_letter(word) \n",
    "        \n",
    "    edit_two_set = edit_two_set | set(tmp)\n",
    "    \n",
    "    # return this as a set instead of a list\n",
    "    return set(edit_two_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 2654\n",
      "First 10 strings ['', 'a', 'aa', 'aaa', 'aab', 'aac', 'aad', 'aae', 'aaf', 'aag']\n",
      "Last 10 strings ['zv', 'zva', 'zw', 'zwa', 'zx', 'zxa', 'zy', 'zya', 'zz', 'zza']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 7154\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"a\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Suggest Spelling Suggestions\n",
    "\n",
    "Now we use`edit_two_letters` function to get a set of all the possible 2 edits on the word. Then use those strings to get the most probable word meant to type a.k.a typing suggestion.\n",
    "\n",
    "**Step 1:** Generate suggestions for a supplied word: we'll use the edit functions. The 'suggestion algorithm' follows this logic: \n",
    "* If the word is in the vocabulary, suggest the word. \n",
    "* Otherwise, if there are suggestions from `edit_one_letter` that are in the vocabulary\n",
    "* Otherwise, if there are suggestions from `edit_two_letters` that are in the vocabulary\n",
    "* Otherwise, suggest the input word.*  \n",
    "* The idea is that words generated from fewer edits are more likely than words with more edits.\n",
    "\n",
    "**Step 2**: Create a 'best_words' dictionary where the 'key' is a suggestion and the 'value' is the probability of that word in the vocabulary. If the word is not in the vocabulary, assign it a probability of 0.\n",
    "\n",
    "**Step 3**: Select the n best suggestions. There may be fewer than n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    prob_dict = {}\n",
    "\n",
    "    suggestions = ([word] if word in vocab\n",
    "                    else [w for w in edit_one_letter(word) if w in vocab]\n",
    "                    or [w for w in edit_two_letters(word) if w in vocab]\n",
    "                    or [word])\n",
    "\n",
    "    #Step 3: Get all the best words and return the most probable top n_suggested words as n_best\n",
    "    \n",
    "    sugg_dict = {w: probs.get(w, 0) for w in suggestions}\n",
    "\n",
    "    # Create a Counter dictionary from these values\n",
    "    n_best = sorted(sugg_dict.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  dys \n",
      "suggestions =  ['days', 'dye']\n",
      "word 0: days, probability 0.000410\n",
      "word 1: dye, probability 0.000019\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation - feel free to try other words in my word\n",
    "my_word = 'dys' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "# CODE REVIEW COMMENT: using \"tmp_corrections\" insteads of \"cors\". \"cors\" is not defined\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Edit Distance\n",
    "\n",
    "How do we evaluate the similarity between two strings? For example: 'waht' and 'what'\n",
    "\n",
    "Also how do we efficiently find the shortest path to go from the word, 'waht' to the word 'what'?\n",
    "\n",
    "we will implement a dynamic programming system that will tell the minimum number of edits required to convert a string into another string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below describes how to initialize the table. Each entry in D[i,j] represents the minimum cost of converting string source[0:i] to string target[0:j]. The first column is initialized to represent the cumulative cost of deleting the source characters to convert string \"EER\" to \"\". The first row is initialized to represent the cumulative cost of inserting the target characters to convert from \"\" to \"NEAR\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/EditDistInit4.PNG' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:1000px;height:400px;\"/> Figure 6 Initializing Distance Matrix</div>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the remainder of the table utilizes the 'Per Cell Operations' in the equation (5) above. Note, the diagram below includes in the table some of the 3 sub-calculations shown in light grey. Only 'min' of those operations is stored in the table in the `min_edit_distance()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/EditDistFill2.PNG' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:800px;height:400px;\"/> Figure 7 Filling Distance Matrix</div>     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the formula for $D[i,j]$ shown in the image is equivalent to:\n",
    "\n",
    "\\begin{align}\n",
    " \\\\\n",
    "D[i,j] =min\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del\\_cost\\\\\n",
    "D[i,j-1] + ins\\_cost\\\\\n",
    "D[i-1,j-1] + \\left\\{\\begin{matrix}\n",
    "rep\\_cost; & if src[i]\\neq tar[j]\\\\\n",
    "0 ; & if src[i]=tar[j]\n",
    "\\end{matrix}\\right.\n",
    "\\end{cases}\n",
    "\\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "The variable `sub_cost` (for substitution cost) is the same as `rep_cost`; replacement cost.  We will stick with the term \"replace\" whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some examples of cells where replacement is used. This also shows the minimum path from the lower right final position where \"EER\" has been replaced by \"NEAR\" back to the start. This provides a starting point for the optional 'backtrace' algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/EditDistExample1.PNG' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:1200px;height:400px;\"/> Figure 8 Examples Distance Matrix</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string we are starting with\n",
    "        target: a string corresponding to the string we want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "    # use deletion and insert cost as  1\n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    #initialize cost matrix with zeros and dimensions (m+1,n+1) \n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "     \n",
    "\n",
    "    for row in range(1,m+1): # Replace None with the proper range\n",
    "        D[row,0] = D[row-1,0]+del_cost\n",
    "        \n",
    "\n",
    "    for col in range(1,n+1): # Replace None with the proper range\n",
    "        D[0,col] = D[0,col-1]+ins_cost\n",
    "        \n",
    "    # Loop through row 1 to row m, both inclusive\n",
    "    for row in range(1,m+1):\n",
    "        \n",
    "        # Loop through column 1 to column n, both inclusive\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # Intialize r_cost to the 'replace' cost \n",
    "            r_cost = rep_cost         \n",
    "            # Check to see if source character at the previous row\n",
    "            # matches the target character at the previous column, \n",
    "            if source[row-1]==target[col-1]: # Replace None with a proper comparison\n",
    "                # Update the replacement cost to 0 if source and target are the same\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Update the cost at row, col based on previous entries in the cost matrix\n",
    "            # Refer to the equation calculate for D[i,j] (the minimum of three calculated costs)\n",
    "            D[row,col] = min(D[row-1,col]+del_cost,D[row,col-1]+ins_cost,D[row-1,col-1]+r_cost)\n",
    "    # Set the minimum edit distance with the cost found at row m, column n \n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**  \n",
    "```CPP\n",
    "(empty)\n",
    "```\n",
    "\n",
    "The 'replace()' routine utilizes all letters a-z one of which returns the original word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eer eer 0\n"
     ]
    }
   ],
   "source": [
    "source = \"eer\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits != 2 and min_edits != 1: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Results**  \n",
    "```CPP\n",
    "eer eer 0\n",
    "```\n",
    "\n",
    "We have to allow single edits here because some two_edits will restore a single edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750f018e",
   "metadata": {},
   "source": [
    "#### References\n",
    "- Dan Jurafsky - Speech and Language Processing - Textbook\n",
    "- This auto-correct explanation was first done by Peter Norvig in 2007 "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
